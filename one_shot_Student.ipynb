{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One Shot Learning with Siamese Networks\n",
    "\n",
    "Adapted from: https://github.com/harveyslash/Facial-Similarity-with-Siamese-Networks-in-Pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will be using a Siamese Network to perform a image verification task on the Omniglot dataset. Given two images, we will determine if they belong to the same class. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import torchvision\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader,Dataset\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.utils\n",
    "import numpy as np\n",
    "import random\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import PIL.ImageOps    \n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow(img,text=None,should_save=False):\n",
    "    npimg = img.numpy()\n",
    "    plt.axis(\"off\")\n",
    "    if text:\n",
    "        plt.text(75, 8, text, style='italic',fontweight='bold',\n",
    "            bbox={'facecolor':'white', 'alpha':0.8, 'pad':10})\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()    \n",
    "\n",
    "def show_plot(iteration,loss):\n",
    "    plt.plot(iteration,loss)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dir = \"./data/training/\"\n",
    "testing_dir = \"./data/testing/\"\n",
    "train_batch_size = 64\n",
    "train_number_epochs = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Dataset Class\n",
    "This dataset returns a pair of images and a label 1 if they belong to different classes, or 0 if they belong to the same class. The images are chosen so roughly 50% of the images in a pair belong to the same class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiameseNetworkDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, imageFolderDataset, sampleEvenly = False):\n",
    "        self.imageFolderDataset = imageFolderDataset   \n",
    "        self.images = np.array(list(map(list, self.imageFolderDataset.imgs)))\n",
    "        self.sampleEvenly = sampleEvenly\n",
    "        self.i = 0\n",
    "        \n",
    "    def __getitem__(self,index):\n",
    "        img0_tuple = random.choice(self.images)\n",
    "        #we need to make sure approx 50% of images are in the same class\n",
    "        should_get_same_class = random.randint(0,1)\n",
    "        if should_get_same_class:\n",
    "            ind = random.choice(np.where(self.images[:,1] == img0_tuple[1])[0])\n",
    "            img1_tuple = self.images[ind]\n",
    "        else:\n",
    "            img1_tuple = random.choice(self.images)\n",
    "        if not self.sampleEvenly:\n",
    "            img1_tuple = self.images[self.i]\n",
    "            self.i += 1\n",
    "        \n",
    "        img0 = Image.open(img0_tuple[0])\n",
    "        img1 = Image.open(img1_tuple[0])\n",
    "        img0 = img0.convert(\"L\")\n",
    "        img1 = img1.convert(\"L\")\n",
    "        \n",
    "        transform=transforms.Compose([transforms.Resize((100,100)),transforms.ToTensor()])\n",
    "        \n",
    "        img0 = transform(img0)\n",
    "        img1 = transform(img1)\n",
    "\n",
    "        return img0, img1 , torch.from_numpy(np.array([int(img1_tuple[1]!=img0_tuple[1])],dtype=np.float32))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.imageFolderDataset.imgs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Image Folder Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_dataset = dset.ImageFolder(root=training_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "siamese_dataset = SiameseNetworkDataset(imageFolderDataset=folder_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualising some of the data\n",
    "The top row and the bottom row of any column is one pair. The 0s and 1s correspond to the column of the image.\n",
    "0 indiciates dissimilar, and 1 indicates similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis_dataloader = DataLoader(siamese_dataset,\n",
    "                        shuffle=True,\n",
    "                        num_workers=8,\n",
    "                        batch_size=8)\n",
    "dataiter = iter(vis_dataloader)\n",
    "\n",
    "example_batch = next(dataiter)\n",
    "concatenated = torch.cat((example_batch[0],example_batch[1]),0)\n",
    "imshow(torchvision.utils.make_grid(concatenated))\n",
    "print(example_batch[2].numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Neural Net Definition\n",
    "We will use a standard convolutional neural network with the following structure (Note the input image has 1 channel):\n",
    "*  Reflection padding to maintain size\n",
    "*  Conv Layer with 4 3x3 kernels\n",
    "*  ReLU\n",
    "*  BatchNorm - takes in number of features (number of output channels of conv layer)\n",
    "\n",
    "*  Reflection padding to maintain size\n",
    "*  Conv Layer with 8 3x3 kernels\n",
    "*  ReLU\n",
    "*  BatchNorm\n",
    "\n",
    "*  Reflection padding to maintain size\n",
    "*  Conv Layer with 8 3x3 kernels\n",
    "*  ReLU\n",
    "*  BatchNorm\n",
    "\n",
    "\n",
    "The Conv2d function takes in as input: number of input channels, number of filters (number of output channels), kernel size.\n",
    "\n",
    "\n",
    "Then we will be adding some fully connected layers to bring each image into a 5 dimensional feature space.\n",
    "\n",
    "#### TODO:\n",
    "The first set of layers in the CNN have been added. Using those as reference for syntax, fill in the rest of the layers.\n",
    "\n",
    "Think about what the forward pass of this network should do. What should the inputs and outputs be?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiameseNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        TODO: Fill in the remaining layers of the CNN\n",
    "        \"\"\"\n",
    "        super(SiameseNetwork, self).__init__()\n",
    "        self.cnn1 = nn.Sequential(\n",
    "            nn.ReflectionPad2d(1), # padding with values at edges being reflected\n",
    "            nn.Conv2d(1, 4, kernel_size=3),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.BatchNorm2d(4),\n",
    "            \n",
    "            ...\n",
    "        )\n",
    "\n",
    "        self.fc1 = nn.Sequential(\n",
    "            nn.Linear(8*100*100, 500),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            nn.Linear(500, 500),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            nn.Linear(500, 5))\n",
    "\n",
    "    def forward_once(self, x):\n",
    "        \"\"\"\n",
    "        TODO: Think about how given an input we can appropriately run it through the network. \n",
    "        Is there something we have to do between the convolution and fully connected layers?\n",
    "        \"\"\"\n",
    "        ...\n",
    "        return output\n",
    "\n",
    "    def forward(self, input1, input2):\n",
    "        \"\"\"\n",
    "        TODO: Using the forward_once function (which runs the conv and fully connected layers on a single input)\n",
    "        what should the forward pass of the overall network do? Refer back to the diagram of a Siamese network\n",
    "        and think about the inputs and outputs.\n",
    "        \"\"\"\n",
    "        ...\n",
    "        return euclidean_distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contrastive Loss\n",
    "\n",
    "The contrastive loss function takes in \n",
    "\n",
    "1) The pairwise distance between two images\n",
    "\n",
    "2) Label representing if the images are same (0) or different (1)\n",
    "\n",
    "#### TODO:\n",
    "Implement the loss function:\n",
    "$$L(d, y) = (1-y)*d^2 + y*(\\max(\\alpha - d, 0))^2$$\n",
    "\n",
    "Hints:\n",
    "* Use torch.pow(input, exponent) to square a tensor\n",
    "* Use torch.clamp(input, min=...) or torch.clamp(expression, max=...) to cut off the values of an input at some min or max value\n",
    "* Remember: The input will be given as a batch, but we want a single scalar for the loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContrastiveLoss(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Contrastive loss function.\n",
    "    Based on: http://yann.lecun.com/exdb/publis/pdf/hadsell-chopra-lecun-06.pdf\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, margin=2.0):\n",
    "        super(ContrastiveLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "\n",
    "    def forward(self, euclidean_distance, label):\n",
    "        \"\"\"\n",
    "        TODO: Implement the loss function above\n",
    "        \"\"\"\n",
    "        ...\n",
    "\n",
    "        return loss_contrastive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Time!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(siamese_dataset,\n",
    "                        shuffle=True,\n",
    "                        num_workers=8,\n",
    "                        batch_size=train_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = SiameseNetwork()\n",
    "criterion = ContrastiveLoss()\n",
    "optimizer = optim.Adam(net.parameters(),lr = 0.0005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = []\n",
    "loss_history = [] \n",
    "iteration_number= 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for epoch in range(0, train_number_epochs):\n",
    "    for i, data in enumerate(train_dataloader,0):\n",
    "        img0, img1 , label = data\n",
    "        img0, img1 , label = Variable(img0), Variable(img1) , Variable(label)\n",
    "        distance = net(img0,img1)\n",
    "        optimizer.zero_grad()\n",
    "        loss_contrastive = criterion(distance,label)\n",
    "        loss_contrastive.backward()\n",
    "        optimizer.step()\n",
    "        if i %10 == 0 :\n",
    "            print(\"Epoch number {}\\n Current loss {}\\n\".format(epoch,loss_contrastive.data[0]))\n",
    "            iteration_number +=10\n",
    "            counter.append(iteration_number)\n",
    "            loss_history.append(loss_contrastive.data[0])\n",
    "show_plot(counter,loss_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some simple testing\n",
    "The last 3 subjects were held out from the training, and will be used to test. The Distance between each image pair denotes the degree of similarity the model found between the two images. Less means it found more similar, while higher values indicate it found them to be dissimilar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiameseNetworkDataset_Evaluation():\n",
    "    \n",
    "    def __init__(self, imageFolderDataset):\n",
    "        self.imageFolderDataset = imageFolderDataset   \n",
    "        self.images = np.array(list(map(list, self.imageFolderDataset.imgs)))\n",
    "        self.ind = 0\n",
    "        \n",
    "    def __iter__(self):\n",
    "        return self\n",
    "        \n",
    "    def next(self):\n",
    "        img0_tuple = random.choice(self.images)\n",
    "        img1_tuple = self.images[self.ind]\n",
    "        self.ind += 1\n",
    "        \n",
    "        img0 = Image.open(img0_tuple[0])\n",
    "        img1 = Image.open(img1_tuple[0])\n",
    "        img0 = img0.convert(\"L\")\n",
    "        img1 = img1.convert(\"L\")\n",
    "        \n",
    "        transform=transforms.Compose([transforms.Resize((100,100)),transforms.ToTensor()])\n",
    "        \n",
    "        img0 = transform(img0)\n",
    "        img1 = transform(img1)\n",
    "        return img0, img1, img0_tuple[1], img1_tuple[1]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.imageFolderDataset.imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "marked": true
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "folder_dataset_test = dset.ImageFolder(root=testing_dir)\n",
    "dataiter = SiameseNetworkDataset_Evaluation(imageFolderDataset=folder_dataset_test)\n",
    "\n",
    "x0,_,x0_label, _= dataiter.next()\n",
    "\n",
    "\n",
    "\n",
    "images = []\n",
    "dissimilarity = []\n",
    "x1_labels = []\n",
    "for i in range(len(folder_dataset_test.imgs)-1):\n",
    "    _,x1, _, x1_label = dataiter.next()\n",
    "    x1_labels.append(x1_label)\n",
    "    x0 = x0.view(1, 1, 100, 100)\n",
    "    x1 = x1.view(1, 1, 100, 100)\n",
    "    concatenated = torch.cat((x0,x1),0)\n",
    "    euclidean_distance = net(Variable(x0),Variable(x1))\n",
    "    images.append(torchvision.utils.make_grid(concatenated))\n",
    "    dissimilarity.append(euclidean_distance.data.numpy()[0][0]) \n",
    "    imshow(images[i], 'Dissimilarity: {:.2f}'.format(dissimilarity[i]))\n",
    "    \n",
    "dissimilarity = np.array(dissimilarity)\n",
    "x1_labels = np.array(x1_labels)\n",
    "score = 0\n",
    "num_same = sum(x1_labels == x0_label)\n",
    "\n",
    "pred_same = dissimilarity.argsort()[:num_same]\n",
    "score += sum([0]*num_same == (x1_labels[pred_same] != [x0_label]*num_same))\n",
    "num_diff = len(x1_labels) - num_same\n",
    "pred_diff = dissimilarity.argsort()[num_same:]\n",
    "score += sum([1]*num_diff == (x1_labels[pred_diff] != [x0_label]*num_diff))\n",
    "print(\"Accuracy: \", score/len(dissimilarity))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
